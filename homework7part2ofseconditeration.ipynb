{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gferew1/IntroML/blob/main/homework7part2ofseconditeration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdXMAx6cceLe",
        "outputId": "e4afee65-20b4-4dfa-8a23-9c7ee40b6be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Check for CUDA availability and set the device\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for question 2 the 2nd of 3 iterations"
      ],
      "metadata": {
        "id": "nKF9o7YY8cdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResNet block\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-10\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layers = self._make_layers(ResNetBlock, 64, 10, stride=1)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, 10)  # CIFAR-10 has 10 classes\n",
        "\n",
        "    def _make_layers(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Create ResNet-10 model\n",
        "model = ResNet10().to(device)\n",
        "\n",
        "# Loss and optimizer with weight decay\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(300):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/300], Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "end_time = time.time()\n",
        "total_training_time = end_time - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "\n",
        "# Report the training time, loss, and accuracy\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Final Training Loss: {running_loss / len(train_loader)}\")\n",
        "print(f\"Test Accuracy: {accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JESt_vjQc0SP",
        "outputId": "59a18ea4-c58f-4cc5-bcb0-5733f2a12bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44318321.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/300], Loss: 1.4589160615221008\n",
            "Epoch [2/300], Loss: 1.0629309109409752\n",
            "Epoch [3/300], Loss: 0.9194271140696143\n",
            "Epoch [4/300], Loss: 0.8261904657039496\n",
            "Epoch [5/300], Loss: 0.7495093525523115\n",
            "Epoch [6/300], Loss: 0.6790919938050878\n",
            "Epoch [7/300], Loss: 0.6261995939342567\n",
            "Epoch [8/300], Loss: 0.5845073655895565\n",
            "Epoch [9/300], Loss: 0.5480314926113314\n",
            "Epoch [10/300], Loss: 0.5178459560322335\n",
            "Epoch [11/300], Loss: 0.49607181503339803\n",
            "Epoch [12/300], Loss: 0.46853169821717244\n",
            "Epoch [13/300], Loss: 0.4532507176289473\n",
            "Epoch [14/300], Loss: 0.4359649487621034\n",
            "Epoch [15/300], Loss: 0.4218466270457753\n",
            "Epoch [16/300], Loss: 0.41118787930292244\n",
            "Epoch [17/300], Loss: 0.3952869180675663\n",
            "Epoch [18/300], Loss: 0.384347321737148\n",
            "Epoch [19/300], Loss: 0.3764310587016518\n",
            "Epoch [20/300], Loss: 0.36497904066844367\n",
            "Epoch [21/300], Loss: 0.3585716285135435\n",
            "Epoch [22/300], Loss: 0.3472706419046578\n",
            "Epoch [23/300], Loss: 0.3426418005276824\n",
            "Epoch [24/300], Loss: 0.333472288173178\n",
            "Epoch [25/300], Loss: 0.32248260439051996\n",
            "Epoch [26/300], Loss: 0.322218134389509\n",
            "Epoch [27/300], Loss: 0.3110825615313352\n",
            "Epoch [28/300], Loss: 0.3034193484908175\n",
            "Epoch [29/300], Loss: 0.2963058129524636\n",
            "Epoch [30/300], Loss: 0.2943458597525916\n",
            "Epoch [31/300], Loss: 0.28620392457603494\n",
            "Epoch [32/300], Loss: 0.28602686985526854\n",
            "Epoch [33/300], Loss: 0.2801163352816306\n",
            "Epoch [34/300], Loss: 0.27997872778369337\n",
            "Epoch [35/300], Loss: 0.2636750884678053\n",
            "Epoch [36/300], Loss: 0.2685736946742553\n",
            "Epoch [37/300], Loss: 0.26283650948187276\n",
            "Epoch [38/300], Loss: 0.25950594715145237\n",
            "Epoch [39/300], Loss: 0.2544355087763513\n",
            "Epoch [40/300], Loss: 0.2534438117462046\n",
            "Epoch [41/300], Loss: 0.24678983693689946\n",
            "Epoch [42/300], Loss: 0.24645962196466564\n",
            "Epoch [43/300], Loss: 0.2411825977780325\n",
            "Epoch [44/300], Loss: 0.23544097529805225\n",
            "Epoch [45/300], Loss: 0.2399713843489242\n",
            "Epoch [46/300], Loss: 0.23469143553310648\n",
            "Epoch [47/300], Loss: 0.2301553896702159\n",
            "Epoch [48/300], Loss: 0.23062563367435696\n",
            "Epoch [49/300], Loss: 0.2304103597998619\n",
            "Epoch [50/300], Loss: 0.2279857118683093\n",
            "Epoch [51/300], Loss: 0.222576334443696\n",
            "Epoch [52/300], Loss: 0.22407873016794014\n",
            "Epoch [53/300], Loss: 0.2193073325830957\n",
            "Epoch [54/300], Loss: 0.22039275422044421\n",
            "Epoch [55/300], Loss: 0.2126922317782936\n",
            "Epoch [56/300], Loss: 0.2130625319983953\n",
            "Epoch [57/300], Loss: 0.21254694528515686\n",
            "Epoch [58/300], Loss: 0.2121420991237816\n",
            "Epoch [59/300], Loss: 0.20778613987252537\n",
            "Epoch [60/300], Loss: 0.21296897151357377\n",
            "Epoch [61/300], Loss: 0.2020481549908438\n",
            "Epoch [62/300], Loss: 0.2056442855683434\n",
            "Epoch [63/300], Loss: 0.2070395684684329\n",
            "Epoch [64/300], Loss: 0.19911349002662523\n",
            "Epoch [65/300], Loss: 0.20449144358906296\n",
            "Epoch [66/300], Loss: 0.198847368188069\n",
            "Epoch [67/300], Loss: 0.19757562523226604\n",
            "Epoch [68/300], Loss: 0.19986929414827195\n",
            "Epoch [69/300], Loss: 0.19725076964749094\n",
            "Epoch [70/300], Loss: 0.20216879277201869\n",
            "Epoch [71/300], Loss: 0.1897828772835567\n",
            "Epoch [72/300], Loss: 0.19355475647217782\n",
            "Epoch [73/300], Loss: 0.1903177440128363\n",
            "Epoch [74/300], Loss: 0.1939581411764445\n",
            "Epoch [75/300], Loss: 0.19152637178559437\n",
            "Epoch [76/300], Loss: 0.1869281578970992\n",
            "Epoch [77/300], Loss: 0.19118999335390832\n",
            "Epoch [78/300], Loss: 0.1844460554706776\n",
            "Epoch [79/300], Loss: 0.18929503124464503\n",
            "Epoch [80/300], Loss: 0.1864844148649889\n",
            "Epoch [81/300], Loss: 0.18657595052591067\n",
            "Epoch [82/300], Loss: 0.18905869044382553\n",
            "Epoch [83/300], Loss: 0.1802299786025606\n",
            "Epoch [84/300], Loss: 0.18133522900740814\n",
            "Epoch [85/300], Loss: 0.18731965637191786\n",
            "Epoch [86/300], Loss: 0.18206866685767917\n",
            "Epoch [87/300], Loss: 0.18244007685224115\n",
            "Epoch [88/300], Loss: 0.1793870911993029\n",
            "Epoch [89/300], Loss: 0.17802652310761038\n",
            "Epoch [90/300], Loss: 0.1832413727327076\n",
            "Epoch [91/300], Loss: 0.18033251625573848\n",
            "Epoch [92/300], Loss: 0.18288641420128704\n",
            "Epoch [93/300], Loss: 0.18030009576884073\n",
            "Epoch [94/300], Loss: 0.18055583390852678\n",
            "Epoch [95/300], Loss: 0.17477011977864043\n",
            "Epoch [96/300], Loss: 0.1791724369425298\n",
            "Epoch [97/300], Loss: 0.1795221257221211\n",
            "Epoch [98/300], Loss: 0.17418761911523312\n",
            "Epoch [99/300], Loss: 0.17295636428172326\n",
            "Epoch [100/300], Loss: 0.1796856429761328\n",
            "Epoch [101/300], Loss: 0.1737717916174313\n",
            "Epoch [102/300], Loss: 0.17956161300849427\n",
            "Epoch [103/300], Loss: 0.16967614285666924\n",
            "Epoch [104/300], Loss: 0.17264211738048613\n",
            "Epoch [105/300], Loss: 0.17532028772337052\n",
            "Epoch [106/300], Loss: 0.16831957041988593\n",
            "Epoch [107/300], Loss: 0.16762610751649606\n",
            "Epoch [108/300], Loss: 0.17043786010016565\n",
            "Epoch [109/300], Loss: 0.17574584359288825\n",
            "Epoch [110/300], Loss: 0.1627361869339443\n",
            "Epoch [111/300], Loss: 0.16952156363164678\n",
            "Epoch [112/300], Loss: 0.17692693126628467\n",
            "Epoch [113/300], Loss: 0.16605325299493798\n",
            "Epoch [114/300], Loss: 0.1701941335826274\n",
            "Epoch [115/300], Loss: 0.17236068067343338\n",
            "Epoch [116/300], Loss: 0.16479779283523255\n",
            "Epoch [117/300], Loss: 0.16932970146312737\n",
            "Epoch [118/300], Loss: 0.16819900513419409\n",
            "Epoch [119/300], Loss: 0.16815952931904732\n",
            "Epoch [120/300], Loss: 0.1712555488585816\n",
            "Epoch [121/300], Loss: 0.16234952329522204\n",
            "Epoch [122/300], Loss: 0.16445789400421446\n",
            "Epoch [123/300], Loss: 0.16950026571826862\n",
            "Epoch [124/300], Loss: 0.16642589747067302\n",
            "Epoch [125/300], Loss: 0.17100586001868442\n",
            "Epoch [126/300], Loss: 0.166943174236647\n",
            "Epoch [127/300], Loss: 0.15961996856552865\n",
            "Epoch [128/300], Loss: 0.1648894238388142\n",
            "Epoch [129/300], Loss: 0.16599074190916002\n",
            "Epoch [130/300], Loss: 0.16076011711831592\n",
            "Epoch [131/300], Loss: 0.16656036599708335\n",
            "Epoch [132/300], Loss: 0.17095759152756324\n",
            "Epoch [133/300], Loss: 0.16312871180722477\n",
            "Epoch [134/300], Loss: 0.16670035555615753\n",
            "Epoch [135/300], Loss: 0.15977137412904474\n",
            "Epoch [136/300], Loss: 0.1620441495495684\n",
            "Epoch [137/300], Loss: 0.16650302970157865\n",
            "Epoch [138/300], Loss: 0.1671919115840474\n",
            "Epoch [139/300], Loss: 0.16309838086519096\n",
            "Epoch [140/300], Loss: 0.15169303822319222\n",
            "Epoch [141/300], Loss: 0.1742341772407827\n",
            "Epoch [142/300], Loss: 0.15751827829291143\n",
            "Epoch [143/300], Loss: 0.15835880918804643\n",
            "Epoch [144/300], Loss: 0.16284784194453597\n",
            "Epoch [145/300], Loss: 0.16011555243254927\n",
            "Epoch [146/300], Loss: 0.17116012772940614\n",
            "Epoch [147/300], Loss: 0.15693178501389826\n",
            "Epoch [148/300], Loss: 0.1626673707895724\n",
            "Epoch [149/300], Loss: 0.1594932309406645\n",
            "Epoch [150/300], Loss: 0.1603619968018416\n",
            "Epoch [151/300], Loss: 0.16728265791216776\n",
            "Epoch [152/300], Loss: 0.1571595715859052\n",
            "Epoch [153/300], Loss: 0.16122429930340604\n",
            "Epoch [154/300], Loss: 0.1586906978160219\n",
            "Epoch [155/300], Loss: 0.16072194406862758\n",
            "Epoch [156/300], Loss: 0.1558790892705588\n",
            "Epoch [157/300], Loss: 0.16333941972392904\n",
            "Epoch [158/300], Loss: 0.15975732280089117\n",
            "Epoch [159/300], Loss: 0.15932006570879761\n",
            "Epoch [160/300], Loss: 0.16082142092420926\n",
            "Epoch [161/300], Loss: 0.16500877180253454\n",
            "Epoch [162/300], Loss: 0.1505656015423253\n",
            "Epoch [163/300], Loss: 0.15862655562474903\n",
            "Epoch [164/300], Loss: 0.15498596924307095\n",
            "Epoch [165/300], Loss: 0.1623397188837571\n",
            "Epoch [166/300], Loss: 0.1483156956313059\n",
            "Epoch [167/300], Loss: 0.15653342670758666\n",
            "Epoch [168/300], Loss: 0.15457391900860745\n",
            "Epoch [169/300], Loss: 0.15986823915596812\n",
            "Epoch [170/300], Loss: 0.1601805175795122\n",
            "Epoch [171/300], Loss: 0.15879486052467084\n",
            "Epoch [172/300], Loss: 0.15936809339944055\n",
            "Epoch [173/300], Loss: 0.1528495652100924\n",
            "Epoch [174/300], Loss: 0.15804437579363204\n",
            "Epoch [175/300], Loss: 0.16185587137708884\n",
            "Epoch [176/300], Loss: 0.1512705837483601\n",
            "Epoch [177/300], Loss: 0.150086299909274\n",
            "Epoch [178/300], Loss: 0.16067972560139263\n",
            "Epoch [179/300], Loss: 0.15317578846231447\n",
            "Epoch [180/300], Loss: 0.15766291465143414\n",
            "Epoch [181/300], Loss: 0.1567351250811611\n",
            "Epoch [182/300], Loss: 0.15258116678089437\n",
            "Epoch [183/300], Loss: 0.16087354516701016\n",
            "Epoch [184/300], Loss: 0.15586951584614755\n"
          ]
        }
      ]
    }
  ]
}